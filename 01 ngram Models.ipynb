{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"01 ngram Models.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"AL8T2iZUN2Qj"},"source":["# n-gram Language Models \n","Traditional ngram language models based purely on straightforward statisics \n","\n","We recommend runing this notebook on Google Colab instead of your local computer to avoid the hassle of installing necessary Python packages on local machine. This notebook does not need \"GPU\" to run, so setting Runtime to \"None\" (CPU) is good enough. If you want to change the runtime, go to <TT>Runtime > Change Runtime Type</TT> and make change from the dropdown menu..\n","\n","We will train the n-gram language models on WikiText-2. A raw version of the data can easily be viewed here: https://github.com/pytorch/examples/tree/master/word_language_model/data/wikitext-2.\n"]},{"cell_type":"markdown","metadata":{"id":"cN2Ja8MNP4qS"},"source":["## Preprocessing the Data\n","\n","To make the models more robust, it is necessary to perform some basic preprocessing on the corpora. \n","\n","* <b>Sentence splitting:</b>&nbsp;&nbsp;&nbsp;&nbsp; identify individual sentences by splitting paragraphs WikiTest dataset at punctuation tokens (\".\",  \"!\",  \"?\").\n","\n","* <b>Sentence markers:</b>&nbsp;&nbsp;&nbsp;&nbsp;For both training and testing corpora, each sentence must be surrounded by a start-of-sentence (`<s>`) and end-of-sentence marker (`/s`). \n","\n","* <b>Unknown words:</b>&nbsp;&nbsp;&nbsp;&nbsp;In order to deal with unknown words in the test corpora, all words that do not appear in the vocabulary must be replaced with a special token for unknown words (`<UNK>`) The WikiText dataset has already done this.\n"]},{"cell_type":"code","metadata":{"id":"FCkrUjKEBrNp"},"source":["# Constants \n","START = \"<s>\"   # Start-of-sentence token\n","END = \"</s>\"    # End-of-sentence-token\n","UNK = \"<UNK>\"   # Unknown word token"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vUdZstjH30DL","executionInfo":{"status":"ok","timestamp":1646607024032,"user_tz":360,"elapsed":9497,"user":{"displayName":"Henry Jiang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06659551641654316375"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"85ee3d6b-1329-4643-e1b7-c658905eaafa"},"source":["\n","import torchtext\n","import random\n","def preprocess(data, vocab=None):\n","    final_data = []\n","    lowercase = \"abcdefghijklmnopqrstuvwxyz\"\n","    for paragraph in data:\n","        # Each of the paragraph variable in the for loop is a long string of the text of one paragraph from the dataset\n","        # the following statement will convert the paragraph into a list of individual tokens, filling in UNK when necessary\n","        # the original text in the dataset does contain '<unk>', by eye-inspecting you can see that.\n","        # For example a paragrah \"I eat <unk> apple\" will be converted to ['I', 'eat', UNK, 'apple']\n","        paragraph = [x if x != '<unk>' else UNK for x in paragraph.split()]\n","\n","        if vocab is not None:\n","            # filling in UNK if a token is not in the vocabulary\n","            paragraph = [x if x in vocab else UNK for x in paragraph]\n","        if paragraph == [] or paragraph.count('=') >= 2: continue\n","        sen = []\n","        prev_punct, prev_quot = False, False\n","        for word in paragraph:\n","            if prev_quot:\n","                if word[0] not in lowercase:\n","                    final_data.append(sen)\n","                    sen = []\n","                    prev_punct, prev_quot = False, False\n","            if prev_punct:\n","                if word == '\"':\n","                    prev_punct, prev_quot = False, True\n","                else:\n","                    if word[0] not in lowercase:\n","                        final_data.append(sen)\n","                        sen = []\n","                        prev_punct, prev_quot = False, False\n","            if word in {'.', '?', '!'}: prev_punct = True\n","            sen += [word]\n","        if sen[-1] not in {'.', '?', '!', '\"'}: continue # Prevent a lot of short sentences\n","        final_data.append(sen)\n","    vocab_was_none = vocab is None\n","    if vocab is None:\n","        vocab = set()\n","    for i in range(len(final_data)):\n","        final_data[i] = [START] + final_data[i] + [END]\n","        if vocab_was_none:\n","            for word in final_data[i]:\n","                vocab.add(word)\n","    return final_data, vocab\n","\n","def getDataset():\n","    dataset = torchtext.datasets.WikiText2(root='.data', split=('train', 'valid'))\n","    train_dataset, vocab = preprocess(dataset[0])\n","    test_dataset, _ = preprocess(dataset[1], vocab)\n","\n","    return train_dataset, test_dataset\n","\n","train_dataset, test_dataset = getDataset()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 4.48M/4.48M [00:00<00:00, 28.3MB/s]\n"]}]},{"cell_type":"markdown","metadata":{"id":"mSFJ07ELGUMh"},"source":["Run the next cell to see 10 random sentences of the training data."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"swPwiHBHDDkT","executionInfo":{"status":"ok","timestamp":1646607029982,"user_tz":360,"elapsed":137,"user":{"displayName":"Henry Jiang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06659551641654316375"}},"outputId":"26518bb1-c633-4203-b819-5281c14be935"},"source":["if __name__ == '__main__':\n","    for x in random.sample(train_dataset, 10):\n","        print (x)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['<s>', 'The', 'plan', 'also', 'included', 'the', 'replacement', 'of', 'the', 'two', 'other', 'signaled', 'intersections', 'at', 'West', 'Allenton', 'Road', 'and', 'Oak', 'Hill', 'Road', 'with', 'overpasses', ';', 'the', 'overpass', 'for', 'West', 'Allenton', 'Road', 'is', 'planned', 'to', 'be', 'constructed', 'as', 'a', 'new', 'exit', '4', '.', '</s>']\n","['<s>', 'Venus', 'was', 'important', 'to', 'ancient', 'American', 'civilizations', ',', 'in', 'particular', 'for', 'the', 'Maya', ',', 'who', 'called', 'it', '<UNK>', '<UNK>', ',', '\"', 'the', 'Great', 'Star', '\"', 'or', '<UNK>', '<UNK>', ',', '\"', 'the', 'Wasp', 'Star', '\"', ';', 'they', 'embodied', 'Venus', 'in', 'the', 'form', 'of', 'the', 'god', '<UNK>', '(', 'also', 'known', 'as', 'or', 'related', 'to', '<UNK>', 'and', '<UNK>', 'in', 'other', 'parts', 'of', 'Mexico', ')', '.', '</s>']\n","['<s>', 'His', 'CPR', 'history', 'ends', ',', 'for', 'example', ',', 'with', 'a', 'recounting', 'of', 'Western', 'grievances', 'against', 'economic', 'policies', ',', 'such', 'as', 'high', 'freight', 'rates', 'and', 'the', 'steep', 'import', 'tariffs', 'designed', 'to', 'protect', 'fledgling', 'Canadian', 'manufacturers', '.', '</s>']\n","['<s>', 'The', 'term', 'condom', 'first', 'appears', 'in', 'the', 'early', '18th', 'century', '.', '</s>']\n","['<s>', 'He', 'considered', 'the', 'puzzles', 'to', 'be', '\"', 'brilliantly', 'conceived', '\"', 'and', 'found', 'the', 'game', \"'s\", 'controls', 'accessible', '.', '</s>']\n","['<s>', 'Indeed', ',', 'most', 'are', 'invisible', 'from', 'Earth', 'even', 'through', 'the', 'most', 'powerful', 'telescopes', '.', '</s>']\n","['<s>', 'In', 'New', 'Jersey', ',', 'high', 'winds', 'caused', 'power', 'outages', 'and', 'knocked', 'down', 'trees', 'and', 'power', 'lines', '.', '</s>']\n","['<s>', 'However', ',', 'some', '30', '%', 'of', 'married', 'women', 'of', 'working', 'age', 'were', 'allowed', 'to', 'stay', 'at', 'home', 'as', 'full', '@-@', 'time', 'housewives', '(', 'less', 'than', 'some', 'countries', 'in', 'the', 'same', 'region', 'like', 'South', 'Korean', '\\\\', 'Japan', 'and', 'Taiwan', ',', 'more', 'than', 'Soviet', 'Union', '\\\\', '<UNK>', 'China', 'or', 'Nordic', 'countries', 'like', 'Sweden', ',', 'about', 'the', 'same', 'as', 'Today', \"'s\", 'USA', ')', '.', '</s>']\n","['<s>', 'This', 'was', 'an', 'unheard', '@-@', 'of', 'quantity', 'at', 'the', 'time', ',', 'and', '<UNK>', 'did', 'not', 'take', 'the', 'man', 'seriously', '.', '</s>']\n","['<s>', 'Fey', 'told', 'The', 'New', 'Yorker', ',', '\"', 'I', \"'d\", 'had', 'my', 'eye', 'on', 'the', 'show', 'forever', ',', 'the', 'way', 'other', 'kids', 'have', 'their', 'eye', 'on', 'Derek', '<UNK>', '.', '\"', '</s>']\n"]}]},{"cell_type":"markdown","metadata":{"id":"YM6hNHMqTMt2"},"source":["## The LanguageModel Class\n","\n","4 types of language models will be implemented: a <b>unigram</b> model, a <b>smoothed unigram</b> model, a <b>bigram</b> model, and a <b>smoothed bigram</b> model. <b> This class is just a skeleton from which subclasses will be extended </b>, the following methods will be implemented in each subclass:\n","\n","* <b>`__init__(self, trainCorpus)`</b>: Train the language model on `trainCorpus`. This will involve calculating relative frequency estimates according to the type of model you're implementing.\n","\n","* <b>`generateSentence(self)`</b>: <b></b> Return a sentence that is generated by the language model. It should be a list of the form <TT>[&lt;s&gt;, w<sup>(1)</sup>, ..., w<sup>(n)</sup>, &lt;&sol;s&gt;]</TT>, where each <TT>w<sup>(i)</sup></TT> is a word in the vocabulary (including <TT>&lt;UNK&gt;</TT> but exlcuding <TT>&lt;s&gt;</TT> and <TT>&lt;&sol;s&gt;</TT>). Assume that <TT>&lt;s&gt;</TT> starts each sentence (with probability $1$). The following words <TT>w<sup>(1)</sup></TT>, ... , <TT>w<sup>(n)</sup></TT>, <TT>&lt;&sol;s&gt;</TT> are generated according to the language model's distribution. Note that the number of words <TT>n</TT> is not fixed; instead, the sentence terminates as soon as an end token <TT>&lt;&sol;s&gt;</TT> is generated.\n","\n","* <b>`getSentenceLogProbability(self, sentence)`</b>: <b> </b> Return the <em> naturl logarithm (base-e logarithm) of the probability</em> of <TT>sentence</TT>, which is again a list of the form <TT>[&lt;s&gt; w<sup>(1)</sup>, ..., w<sup>(n)</sup>, &lt;&sol;s&gt;]</TT>. See the note below about performing the calculations in logarithm.\n","\n","* <b>`getCorpusPerplexity(self, testCorpus)`</b>: <b> </b> Compute the perplexity (normalized inverse log probability) of `testCorpus` . For a corpus $W$ with $N$ words and a bigram model, Jurafsky and Martin's book presents a formula to compute perplexity as follows: \n","\n","$$Perplexity(W) = \\Big [ \\prod_{i=1}^N \\frac{1}{P(w^{(i)}|w^{(i-1)})} \\Big ]^{1/N}$$\n","\n","In order to avoid underflow,  do all of calculations in logarithms. That is, instead of multiplying probabilities, we add the logarithms of the probabilities and exponentiate the result:\n","\n","$$\\prod_{i=1}^N P(w^{(i)}|w^{(i-1)}) = \\exp\\Big (\\sum_{i=1}^N \\log P(w^{(i)}|w^{(i-1)}) \\Big ) $$\n","\n","\n"]},{"cell_type":"code","metadata":{"cellView":"code","id":"uKO6dHNSS45P"},"source":["import math\n","from collections import defaultdict\n","\n","class LanguageModel(object):\n","    def __init__(self, trainCorpus):\n","        '''\n","        Initialize and train the model (i.e. estimate the model's underlying probability\n","        distribution from the training corpus.)\n","        '''\n","\n","        return\n","\n","    def generateSentence(self):\n","        '''\n","        Generate a sentence by drawing words according to the model's probability distribution.\n","        '''\n","\n","       \n","        raise NotImplementedError(\"Implement generateSentence in each subclass.\")\n","\n","    def getSentenceLogProbability(self, sentence):\n","        '''\n","        Calculate the log probability of the sentence provided. \n","        '''\n","\n","        raise NotImplementedError(\"Implement getSentenceProbability in each subclass.\")\n","        \n","    def getCorpusPerplexity(self, testCorpus):\n","        '''\n","        Calculate the perplexity of the corpus provided.\n","        '''\n","\n","        raise NotImplementedError(\"Implement getCorpusPerplexity in each subclass.\")\n","\n","    def printSentences(self, n):\n","        '''\n","        Prints n sentences generated by the model.\n","        '''\n","\n","        for i in range(n):\n","            sent = self.generateSentence()\n","            prob = self.getSentenceLogProbability(sent)\n","            print('Log Probability:', prob , '\\tSentence:',sent)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Bf15l6f3etMV"},"source":["## </font> Unigram Model\n","\n","Implement each of the 4 functions described above for an <b>unsmoothed unigram</b> model. The probability distribution of a word is given by $\\hat P(w)$."]},{"cell_type":"code","metadata":{"id":"T2uZdMsqeuf2"},"source":["class UnigramModel(LanguageModel):\n","    def __init__(self, trainCorpus):\n","\n","        print(\"Initiliaze un-smoothed UnigramModel\")\n","        # A dictionary with each key as each token in the vocabulary, and the value as that token's frequency in the coupus\n","        self.counts = defaultdict(int)\n","        # total tokens in the corpus\n","        self.total = 0\n","        \n","        for sent in trainCorpus:\n","            for word in sent:\n","              # exclude counting the start of sentence token <s>\n","                if word != START:\n","                    self.counts[word] += 1\n","                    self.total +=1\n"," \n","    def generateSentence(self):\n","        # the first token of a generated sentence is always the start <s> token\n","        output_sent = [START]\n","        curr_word = START\n","      \n","        # as long as it is not the end of sentence, keep generating the next word\n","        while curr_word != END:\n","            rand = random.random()\n","            cumulative_prob = 0.0\n","            for word in self.counts.keys():\n","               cumulative_prob += float(self.counts[word])/self.total\n","               if rand < cumulative_prob:\n","                  break\n","            curr_word = word;\n","            output_sent.append(word)\n","        return output_sent\n","\n","    def getSentenceLogProbability(self, sentence):\n","        \n","        log_prob = 0.0\n","        for word in sentence:\n","            if word != START:\n","                # add a check to make sure word is in self.counts before accessing it , otherwise nonexisting word will be added to \n","                # self.counts with value 0, that changes the structure of self.counts[word]. This check also serves the purpose of \n","                # sanity check for the case of the word is not in self.counts, i.e it does not appear in the traning set, \n","                # and sentence probability shouls be 0, log probability is then negative infinity. Without the check there \n","                # could be math excpetion for log(0)?               \n","                if word in self.counts.keys():\n","                    temp_count = self.counts[word]\n","                    log_prob += math.log(float(temp_count)/self.total)\n","                else:\n","                    temp_count = 0\n","                    # the probability of sentence is 0 because this word is not in training set, log_prob should be -inf\n","                    log_prob = float(\"-inf\")\n","\n","        return log_prob\n"," \n","\n","      \n","    def getCorpusPerplexity(self, testCorpus):\n","\n","        log_prob = 0.0\n","        count = 0\n","        for sent in testCorpus:\n","            for word in sent:\n","                if word != START:\n","                    count += 1\n","                    log_prob += math.log(float(self.counts[word])/self.total)\n","        return math.exp(-1.0/count * log_prob)\n","        "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-Z8h9U63AkiG"},"source":["Train the model on the full WikiText corpus, and evaluate it on the held-out test set."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s1XHIg0xrUIt","executionInfo":{"status":"ok","timestamp":1646607045494,"user_tz":360,"elapsed":2904,"user":{"displayName":"Henry Jiang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06659551641654316375"}},"outputId":"bb8a3ecc-bd50-40ef-cd56-8224b52fdbb7"},"source":["def runModel(model_type):\n","    assert model_type in {'unigram', 'bigram', 'smoothed-unigram', 'smoothed-bigram'}\n","    # Read the corpora\n","    if model_type == 'unigram':\n","        model = UnigramModel(train_dataset)\n","    elif model_type == 'bigram':\n","        model = BigramModel(train_dataset)\n","    elif model_type == 'smoothed-unigram':\n","        model = SmoothedUnigramModel(train_dataset)\n","    else:\n","        model = SmoothedBigramModelAD(train_dataset)\n","\n","    print(\"--------- 5 sentences from the model ---------\")\n","    model.printSentences(5)\n","\n","    print (\"\\n--------- Corpus Perplexities ---------\")\n","    print (\"Training Set:\", model.getCorpusPerplexity(train_dataset))\n","    print (\"Testing Set:\", model.getCorpusPerplexity(test_dataset))\n","\n","if __name__=='__main__':\n","    runModel('unigram')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Initiliaze un-smoothed UnigramModel\n","--------- 5 sentences from the model ---------\n","Log Probability: -286.2687966073898 \tSentence: ['<s>', 'of', 'was', 'in', 'this', 'United', 'the', '.', 'throughout', 'in', 'remains', 'In', 'where', 'the', 'to', '3', \"'s\", 'nitrous', 'were', 'entrenched', 'may', 'three', ';', 'a', 'availability', '12', 'two', 'Series', 'not', ',', 'Finkelstein', ',', 'depression', 'and', 'Crystal', 'the', '<UNK>', 'value', 'presented', 'Forced', 'during', 'time', 'over', '</s>']\n","Log Probability: -297.928905998917 \tSentence: ['<s>', 'is', 'the', 'was', 'as', '<UNK>', 'his', 'distribution', 'the', 'businesses', '.', 'its', 'with', 'another', 'of', 'had', 'the', 'ones', 'populations', ',', 'farm', 'appeared', '<UNK>', 'in', 'gained', 'worth', 'of', '<UNK>', 'lines', 'Nicaragua', 'SAS', 'advantage', 'technology', ',', 'student', 'five', 'The', 'Moriarty', 'artistic', '@-@', 'United', 'service', 'provided', '</s>']\n","Log Probability: -3.3214585046256566 \tSentence: ['<s>', '</s>']\n","Log Probability: -285.9337571291011 \tSentence: ['<s>', 'and', '.', '\"', '@-@', '...', '.', 'repressive', 'Max', 'Toronto', 'the', 'in', 'about', '35', 'Space', 'She', 'Administrative', 'that', 'session', 'battlefield', '@-@', '.', 'Carnival', 'magazine', ',', 'of', 'Japan', 'to', 'could', 'were', '23', 'band', 'to', 'they', 'an', 'partner', 'sources', 'indefinitely', 'of', 'large', '</s>']\n","Log Probability: -331.5401520938011 \tSentence: ['<s>', 'nation', '1915', 'investigate', 'For', 'middle', ',', 'Crab', 'in', 'never', 'of', 'in', 'other', 'America', '<UNK>', 'style', 'with', 'block', 'Saginaw', 'estuary', 'based', '–', 'may', 'Isabella', 'production', 'the', 'Arnhem', 'on', 'Tsubame', 'he', 'Lords', 'in', 'media', 'structures', 'Highlanders', 'computer', 'and', '.', 'Innis', 'used', 'said', 'this', 'It', '</s>']\n","\n","--------- Corpus Perplexities ---------\n","Training Set: 1101.9435880270637\n","Testing Set: 912.1574385914796\n"]}]},{"cell_type":"markdown","metadata":{"id":"2bGyA8vOfvRj"},"source":["##</font> Smoothed Unigram Model \n","\n","Implement each of the 4 functions described above for a <b>unigram</b> model with <b>Laplace (add-one) smoothing</b>. The probability distribution of a word is given by $P_L(w)$. This type of smoothing takes away some of the probability mass for observed events and assigns it to unseen events.\n","\n","In order to smooth the model, we need the number of words in the corpus, $N$, and the number of word types, $S$. The distinction between these is meaningful: $N$ indicates the number of word instances, where $S$ refers to the size of our vocabulary. For example, the sentence <em>the cat saw the dog</em> has four word types (<em>the</em>, <em>cat</em>, <em>saw</em>, <em>dog</em>), but five word tokens (<em>the</em>, <em>cat</em>, <em>saw</em>, <em>the</em>, <em>dog</em>). The token <em>the</em> appears twice in the sentence, but they share the same type <em>the</em>.\n","\n","If $c(w)$ is the frequency of $w$ in the training data, then compute $P_L(w)$ as follows:\n","\n","$$P_L(w)=\\frac{c(w)+1}{N+S}$$"]},{"cell_type":"code","metadata":{"id":"wzX-UZJPfvRn"},"source":["class SmoothedUnigramModel(LanguageModel):\n","    def __init__(self, trainCorpus):\n","\n","        print(\"Initiliaze smoothed UnigramModel\")\n","        # A dictionary with each key as each token in the vocabulary, and the value as that token's frequency in the coupus\n","        self.counts = defaultdict(int)\n","        # total tokens in the corpus\n","        self.total = 0\n","        \n","        for sent in trainCorpus:\n","            for word in sent:\n","              # exclude counting the start of sentence token <s>\n","                if word != START:\n","                    self.counts[word] += 1\n","                    self.total +=1\n","        # The following covers all of the content of smoothing, therefore the rest of the functions \n","        # do not nneed to change and remain the same as the un-smoothed unigram\n","        self.total += len(self.counts)\n","        for word in self.counts.keys():\n","            self.counts[word] += 1\n","\n","    # remains same as unsmoothed unigram\n","    def generateSentence(self):\n","        # the first token of a generated sentence is always the start = <s> token\n","        output_sent = [START]\n","        curr_word = START\n","      \n","        # as long as it is not the end of sentence, keep generating the next word\n","        while curr_word != END:\n","            rand = random.random()\n","            cumulative_prob = 0.0\n","            for word in self.counts.keys():\n","               cumulative_prob += float(self.counts[word])/self.total\n","               if rand < cumulative_prob:\n","                  break\n","            curr_word = word;\n","            output_sent.append(word)\n","        return output_sent\n","\n","    # remains the same as unsmoothed unigram\n","    def getSentenceLogProbability(self, sentence):\n","        \n","        log_prob = 0.0\n","        for word in sentence:\n","            if word != START:\n","                # add a check to make sure word is in self.counts before accessing it , otherwise nonexisting word will be added to \n","                # self.counts with value 0, that changes the structure of self.counts[word]. This check also serves the purpose of \n","                # sanity check for the case of the word is not in self.counts, i.e it does not appear in the traning set, \n","                # and sentence probability shouls be 0, log probability is then negative infinity. Without the check there \n","                # could be math excpetion for log(0)?               \n","                if word in self.counts.keys():\n","                    temp_count = self.counts[word]\n","                    log_prob += math.log(float(temp_count)/self.total)\n","                else:\n","                    temp_count = 0\n","                    # the probability of sentence is 0 because this word is not in training set, log_prob should be -inf\n","                    log_prob = float(\"-inf\")\n","\n","        return log_prob\n","        \n","    # remains the same as unsmoothed unigram\n","    def getCorpusPerplexity(self, testCorpus):\n","        \n","        log_prob = 0.0\n","        count = 0\n","        for sent in testCorpus:\n","            for word in sent:\n","                if word != START:\n","                    count += 1\n","                    log_prob += math.log(float(self.counts[word])/self.total)\n","        return math.exp(-1.0/count * log_prob)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6agWmpjdCWOt","executionInfo":{"status":"ok","timestamp":1646607059161,"user_tz":360,"elapsed":2817,"user":{"displayName":"Henry Jiang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06659551641654316375"}},"outputId":"41894e7a-9f39-4a3c-c210-76a83dc00d8a"},"source":["if __name__=='__main__':\n","    runModel('smoothed-unigram')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Initiliaze smoothed UnigramModel\n","--------- 5 sentences from the model ---------\n","Log Probability: -68.71720936354858 \tSentence: ['<s>', ',', 'probably', ',', '.', 'the', '1794', 'have', 'H.', '(', 'predecessor', '</s>']\n","Log Probability: -41.084626898741575 \tSentence: ['<s>', 'World', 'modal', 'into', 'Barrow', '</s>']\n","Log Probability: -3.3374709753302536 \tSentence: ['<s>', '</s>']\n","Log Probability: -49.18116681096004 \tSentence: ['<s>', 'the', \"'s\", 'with', 'that', '\"', 'stalled', 'features', 'the', '</s>']\n","Log Probability: -304.36139267616363 \tSentence: ['<s>', 'October', 'Bullet', 'casualties', 'linebacker', 'major', 'the', 'de', '6', 'to', '1919', 'transformed', 'Ruler', 'bones', 'where', 'colony', 'already', 'split', ',', 'and', 'elementary', 'century', ',', 'of', 'and', 'as', 'five', 'large', 'with', 'normal', 'dropped', 'archipelago', '\"', '\"', ',', 'Best', 'teammate', 'preceded', 'rock', '<UNK>', '</s>']\n","\n","--------- Corpus Perplexities ---------\n","Training Set: 1103.0243317913958\n","Testing Set: 914.4724502277727\n"]}]},{"cell_type":"markdown","metadata":{"id":"vGtcWVMGiEGw"},"source":["## </font> Bigram Model \n","\n","Implement each of the 4 functions described above for a <b>unsmoothed bigram</b> model. The probability distribution of a word is given by $\\hat P(w'|w)$. Thus, the probability of $w_i$ is conditioned on $w_{i-1}$."]},{"cell_type":"code","metadata":{"id":"_ojk_q0YiEGx"},"source":["class BigramModel(LanguageModel):\n","    def __init__(self, trainCorpus):\n","\n","        print(\"Initiliaze un-smoothed BigramModel\")\n","        # A dictionary with each key as a bigram, and the value as that bigram's frequency in the coupus\n","        self.bi_counts = defaultdict(int)\n","        # A dictionary with each key as a unigram in the corpus (each unigram is a the leading word of a bigram),\n","        # and the value as that unigram's frequency in the coupus\n","        self.uni_counts = defaultdict(int)\n","\n","        # prev_word should be the leading word of a bigram\n","        prev_word = START\n","\n","        for sent in trainCorpus:\n","            for word in sent:\n","                if word != START:\n","                    # Denote a bigram as \"current word|prev word\", and use that as the key for the dictionary of bigram frequencies\n","                    # for example \"jumps|cat\" indicates a bigram of two words, with the previous word as \"cat\" and the current word as \"jumps\"\n","                    bigram = word + \"|\" + prev_word\n","                 \n","                    # update the frequency count for the bigram\n","                    self.bi_counts[bigram] += 1\n","                  \n","                    # update the unigram frequency count of the prev_word, note that START is also treated as a unigram \n","                    # and counted. This is different from UnigramModel where we did not count START into counts. \n","                    # So when generating sentence, need to make sure the START token cannnot be selected excpet the beginning of sentence\n","                    self.uni_counts[prev_word] +=1\n","                    \n","                    # we intentioanlly add END = \"</s>\" to the uni_counts, otherwise it will not show up as a key of uni_counts\n","                    # we found this when debugging, and then figured out the reason is that we have been adding prev_word as keys\n","                    # but END = \"</s>\" is the last token in a sentence so it can never be a prev_word and therefore it will not \n","                    # show up. We need it to be in the keys of uni_counts since that is considered to be in the vocabulary, \n","                    # and END = \"</s>\" cannot be missing from the vocabulary\n","                    if (word == END):\n","                        self.uni_counts[word] +=1\n","\n","                prev_word = word\n","\n","\n","    def generateSentence(self):\n","        # the first token of a generated sentence is always the START = <s> token\n","        output_sent = [START]\n","        prev_word = START\n","\n","        # as long as it is not the end of sentence, keep generating the next word\n","        while prev_word != END:\n","            rand = random.random()\n","            cumulative_prob = 0.0\n","            for curr_word in self.uni_counts.keys():\n","                # make sure we do not generate a START token in the middle of a sentence\n","                if curr_word != START: \n","                    bigram = curr_word + \"|\" + prev_word\n","                    # check whether the bigranm is in the bi_counts, and then access self.bi_counts[bigram], \n","                    # otherwise nonexisting keys will be addded with value 0, that will change the structure of bi_counts\n","                    if bigram in self.bi_counts.keys():\n","                        temp_count = self.bi_counts[bigram]\n","                        # could temp_count be 0?, it should not be\n","                        if temp_count == 0:\n","                            raise Exception(\"Test generateSentence, temp_count == 0, how could this happen?\")\n","                        cumulative_prob += float(temp_count)/self.uni_counts[prev_word]\n","                        if rand < cumulative_prob:\n","                            break\n","            prev_word = curr_word;\n","            output_sent.append(curr_word)\n","\n","        return output_sent\n","\n","    def getSentenceLogProbability(self, sentence):\n","        log_prob = 0.0\n","        prev_word = START\n","        for curr_word in sentence:\n","            if curr_word != START:\n","                bigram = curr_word + \"|\" + prev_word\n","                # check whether the bigranm is in the bi_counts, and then access self.bi_counts[bigram], \n","                # otherwise nonexisting keys will be addded with value 0, that will change the structure of bi_counts\n","                if bigram in self.bi_counts.keys():\n","                    temp_count = self.bi_counts[bigram] \n","                    # could temp_count be 0?, it should not be\n","                    if temp_count == 0:\n","                        raise Exception(\"Test getSentenceLogProbability, temp_count == 0, how could this happen?\")\n","                    log_prob += math.log(float(temp_count)/self.uni_counts[prev_word])\n","                else:\n","                    # The bigram is not in bi_counts, The probability for this sentence is 0, and the log probabiity is negative infinity\n","                    log_prob = float(\"-inf\")\n","                prev_word = curr_word\n","        return log_prob \n","        \n","    def getCorpusPerplexity(self, testCorpus):\n","        log_prob = 0.0\n","        word_count = 0\n","\n","        for sent in testCorpus:\n","            # when counting number of words for each sentence, need to deduct by 1 \n","            # since each sentence has a START = \"<s>\" at the beginning, and it should not be counted as a word\n","            word_count += (len(sent)-1)\n","            temp = self.getSentenceLogProbability(sent)\n","            if temp == float(\"-inf\"):\n","            # the log probability for one sentence in the corpus is -inf, i.e. the probability of the sentence is 0,\n","            # so the perplexity of the corpus is +inf\n","                return float(\"inf\")\n","            else:\n","                log_prob += temp\n","\n","        return math.exp(-1.0/word_count * log_prob)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bKop-qYKCZO5","executionInfo":{"status":"ok","timestamp":1646607070660,"user_tz":360,"elapsed":5454,"user":{"displayName":"Henry Jiang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06659551641654316375"}},"outputId":"bb19cab2-b7fc-44e2-a709-083cbb1347d0"},"source":["if __name__=='__main__':\n","    runModel('bigram')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Initiliaze un-smoothed BigramModel\n","--------- 5 sentences from the model ---------\n","Log Probability: -68.18187569931654 \tSentence: ['<s>', 'In', 'July', '2007', 'at', 'well', '@-@', '<UNK>', 'had', 'run', 'had', 'just', 'below', 'the', 'world', '.', '</s>']\n","Log Probability: -5.948757402608453 \tSentence: ['<s>', '<UNK>', '.', '</s>']\n","Log Probability: -50.69394925875432 \tSentence: ['<s>', 'He', 'also', 'occur', 'to', 'place', 'alongside', 'Julia', 'Cho', 'and', 'dreams', '.', '</s>']\n","Log Probability: -139.96871040735041 \tSentence: ['<s>', 'Norwegian', 'trains', 'a', 'series', 'on', '3', '–', 'because', 'Tomita', 'demonstrated', 'firing', 'of', 'his', 'tomb', 'in', 'an', '1811', 'he', 'was', 'portrayed', '<UNK>', '23', 'miles', '(', 'I', ',', 'completed', '.', '</s>']\n","Log Probability: -71.4764232973057 \tSentence: ['<s>', 'But', 'what', 'is', 'an', 'individual', 'components', 'shape', 'of', 'design', 'for', 'the', 'Royal', 'Horse', 'Brigades', 'and', 'training', '.', '</s>']\n","\n","--------- Corpus Perplexities ---------\n","Training Set: 76.92394608735728\n","Testing Set: inf\n"]}]},{"cell_type":"markdown","metadata":{"id":"WPBeyKUsfnnW"},"source":["##</font> Smoothed Bigram Model \n","\n","Implement each of the 4 functions described above for a <b>bigram</b> model with <b>absolute discounting</b>. The probability distribution of a word is given by $P_{AD}(w’|w)$.\n","\n","In order to smooth the model, a discounting factor $D$ needs to be computed. If $n_k$ is the number of bigrams $w_1w_2$ that appear exactly $k$ times, $D$ can be computed as: \n","\n","$$D=\\frac{n_1}{n_1+2n_2}$$ \n","\n","For each word $w$, the number of bigram types $ww’$ is computed as follows: \n","\n","$$S(w)=|\\{w’|c(ww’)>0\\}|$$ \n","\n","where $c(ww’)$ is the frequency of $ww’$ in the training data.\n","\n","Finally, $P_{AD}(w’|w)$ is computed as follows: \n","\n","$$P_{AD}(w’|w)=\\frac{\\max \\big (c(ww’)-D,0\\big )}{c(w)}+\\bigg (\\frac{D}{c(w)}\\cdot S(w) \\cdot P_L(w’)\\bigg )$$ \n","\n","where $c(w)$ is the frequency of $w$ in the training data and $P_L(w’)$ is the Laplace-smoothed unigram probability of $w’$."]},{"cell_type":"code","metadata":{"id":"l1klb00wtVtS","colab":{"base_uri":"https://localhost:8080/","height":243},"executionInfo":{"status":"error","timestamp":1647369793400,"user_tz":300,"elapsed":356,"user":{"displayName":"Henry Jiang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06659551641654316375"}},"outputId":"949f5412-7b80-491e-958f-4367bd3d752a"},"source":["class SmoothedBigramModelAD(LanguageModel):\n","    def __init__(self, trainCorpus):\n","\n","        print(\"Initiliaze smoothed BigramModel\")\n","        # The following block of code is almost exact copy from the unsmoothed model, go there to see comments if needed\n","        self.bi_counts = defaultdict(int)\n","        self.uni_counts = defaultdict(int)\n","        prev_word = START\n","        # total tokens in the corpus\n","        self.total = 0\n","\n","        # The S(w) as defined above, the bigram type\n","        self.S = defaultdict(int)\n","\n","        for sent in trainCorpus:\n","            for word in sent:\n","                if word != START:\n","                    #bigram_count += 1\n","                    bigram = word + \"|\" + prev_word\n","                    #print(prev_word + \"|\" + word)\n","                    # This is the first time this type of bigram ww' appears, \n","                    # so add 1 to the bigram type for the the work \"w\", i.e. the prev_word\n","                    if self.bi_counts[bigram] == 0:\n","                        self.S[prev_word] += 1;\n","                    self.bi_counts[bigram] += 1\n","                    self.uni_counts[prev_word] +=1\n","                    # make sure END = \"</s>\" is also inlcuded into the uni_counts\n","                    # when we generate sentence or calculate P_L(w'), \"</s>\" needs to be included\n","                    if (word == END):\n","                        self.uni_counts[word] +=1\n","                    \n","                    self.total += 1\n","                prev_word = word\n","\n","        n1 = 0\n","        n2 = 0\n","\n","        # in the following bigram with the first word as START = \"<s>\" is also taken into account\n","        for bigram in self.bi_counts.keys():\n","            if self.bi_counts[bigram] == 1:\n","                n1 += 1\n","            elif self.bi_counts[bigram] == 2:\n","                n2 += 1\n","        self.D = float(n1)/(n1 + 2 * n2)\n","                   \n","        \n","    def Prob_Bigram_AD(self, word, prev_word):\n","        # P_L(w'), Laplace smoothed unigram probability. In the following denominator, we take the keys of uni_counts\n","        # as the vocabulary, but it includes START = \"<s>\", and the original Laplace smoothed unigram model does not \n","        # include START = \"<s>\" in the vocabulary, so we need to reduce the vocabulary size by 1, i.e: len(self.uni_counts)-1\n","\n","        P_L = float(self.uni_counts[word]+1)/(self.total + len(self.uni_counts)-1)\n","\n","        bigram = word+\"|\"+prev_word\n","        # check whether the bigranm is in the bi_counts before accessing self.bi_counts[bigram], \n","        # otherwise nonexisting keys will be addded with value 0, that will change the structure of bi_counts\n","        if bigram in self.bi_counts.keys():\n","            temp_count = self.bi_counts[bigram]\n","            # could temp_count be 0?, it should not be\n","            if temp_count == 0:\n","                raise Exception(\"Inside Prob_Bigram_AD, temp_count == 0, how could this happen?\")\n","        else:\n","            temp_count = 0\n","        \n","        return max(temp_count - self.D, 0)/self.uni_counts[prev_word] + self.D * self.S[prev_word]*P_L/self.uni_counts[prev_word]\n","\n","\n","    def generateSentence(self):\n","        # This is almost straight copy from the unsmoothed model, woth only change on probability calculation\n","        # the first token of a generated sentence is always the START = <s> token\n","        \n","        output_sent = [START]\n","        prev_word = START\n","        # as long as it is not the end of sentence, keep generating the next word\n","         \n","        while prev_word != END:\n","            rand = random.random()\n","            cumulative_prob = 0.0\n","            for curr_word in self.uni_counts.keys():\n","                # make sure we do not generate a START token in the middle of a sentence\n","                if curr_word != START: \n","                    cumulative_prob += self.Prob_Bigram_AD(curr_word, prev_word)\n","                    if rand < cumulative_prob:\n","                        break\n","            prev_word = curr_word;\n","            output_sent.append(curr_word)\n","        return output_sent\n","\n","    def getSentenceLogProbability(self, sentence):\n","        log_prob = 0.0\n","        prev_word = START\n","        for curr_word in sentence:\n","            if curr_word != START:\n","                bigram_prob = self.Prob_Bigram_AD(curr_word, prev_word)\n","                if bigram_prob == 0.0:\n","                # The probability for this sentence is 0, and the log probabiity is negative infinity\n","                    return float(\"-inf\")\n","                else:\n","                    log_prob += math.log(bigram_prob)\n","                prev_word = curr_word\n","        \n","        return log_prob\n","        \n","    def getCorpusPerplexity(self, testCorpus):\n","        log_prob = 0.0\n","        word_count = 0\n","\n","        prev_word = START\n","        for sent in testCorpus:\n","            for word in sent:\n","                 if word != START:\n","                    word_count += 1\n","                    bigram_prob = self.Prob_Bigram_AD(word, prev_word)\n","                    if bigram_prob == 0.0:\n","                        # The probability for this sentence is 0, the perplexity of this corpus is infinity\n","                        return float(\"inf\")\n","                    else:\n","                        log_prob += math.log(bigram_prob)\n","                 prev_word = word\n","        return math.exp(-1.0/word_count * log_prob)\n","\n","        \n","\n"],"execution_count":1,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-75c772b6454d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mSmoothedBigramModelAD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLanguageModel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainCorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Initiliaze smoothed BigramModel\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;31m# The following block of code is almost exact copy from the unsmoothed model, go there to see comments if needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'LanguageModel' is not defined"]}]},{"cell_type":"code","metadata":{"id":"GibKGwdXtiUQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1646607089975,"user_tz":360,"elapsed":9713,"user":{"displayName":"Henry Jiang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06659551641654316375"}},"outputId":"e8a21bd4-e61c-4541-f91d-23d852c69640"},"source":["if __name__=='__main__':\n","    runModel('smoothed-bigram')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Initiliaze smoothed BigramModel\n","--------- 5 sentences from the model ---------\n","Log Probability: -26.780832825267428 \tSentence: ['<s>', 'The', 'series', 'finale', 'of', 'books', '.', '</s>']\n","Log Probability: -166.29245216126085 \tSentence: ['<s>', 'The', '<UNK>', 'in', 'a', 'due', 'to', 'provide', 'more', ',', 'she', 'felt', 'that', 'had', 'approached', 'that', 'he', 'was', 'born', 'was', 'involved', 'with', 'these', 'is', 'just', 'a', 'composite', '2008', ',', 'the', 'town', '<UNK>', ',', 'inside', ',', '2007', '.', '</s>']\n","Log Probability: -66.55685142242773 \tSentence: ['<s>', 'The', 'album', 'cover', 'Minor', '<UNK>', ',', 'under', 'match', ',', 'but', 'era', '.', '</s>']\n","Log Probability: -86.21039496077162 \tSentence: ['<s>', 'Measuring', 'deciduous', 'and', 'publisher', 'was', 'also', 'a', 'popular', 'singer', 'and', 'in', 'her', 'visit', 'the', '<UNK>', '.', '</s>']\n","Log Probability: -8.659715625266967 \tSentence: ['<s>', 'He', '.', '</s>']\n","\n","--------- Corpus Perplexities ---------\n","Training Set: 98.55812920555358\n","Testing Set: 272.5720197928467\n"]}]}]}